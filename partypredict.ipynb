{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, Dense, Dropout, Bidirectional, SimpleRNN, Reshape\n",
    "from keras.models import Model, Sequential\n",
    "from keras import callbacks\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "import spacy\n",
    "import numpy as np\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "### opening, reading, and closing .txt files ###\n",
    "demtext = open(\"C:/Users/nickp/OneDrive/Desktop/PartyPred/Democrat/dem.txt\", 'r', encoding = 'utf-8')\n",
    "reptext = open(\"C:/Users/nickp/OneDrive/Desktop/PartyPred/Republican/repub.txt\", 'r', encoding = 'utf-8')\n",
    "rawtext = sent_tokenize(demtext.read())\n",
    "dem_numsents = len(rawtext)\n",
    "for sent in sent_tokenize(reptext.read()):\n",
    "    rawtext.append(sent)\n",
    "rep_numsents = len(rawtext) - dem_numsents\n",
    "\n",
    "demtext.close()\n",
    "reptext.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating training/test splits at 80% training, 20% test ###\n",
    "## note:  data is later split into training/validation/testing at 60/20/20 ##\n",
    "labels = []\n",
    "for i in range(0,dem_numsents):\n",
    "    labels.append(0)\n",
    "for i in range(0, rep_numsents):\n",
    "    labels.append(1)\n",
    "alldata = np.column_stack((rawtext, labels))\n",
    "np.random.shuffle(alldata)\n",
    "cutoff = int(0.8*len(labels))\n",
    "trainsents = alldata[:cutoff, 0]\n",
    "trainlabels = alldata[:cutoff, 1]\n",
    "testsents = alldata[cutoff:,0]\n",
    "testlabels = alldata[cutoff:, 1]\n",
    "trainlabels = trainlabels.astype(np.float)\n",
    "testlabels = testlabels.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating encoder with vocab_size = # tokens ###\n",
    "vocab_size = 14290\n",
    "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens = vocab_size)\n",
    "encoder.adapt(trainsents)\n",
    "vocab = np.array(encoder.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple first-run model using Tensorflow embedding ###\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=64,\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "earlystop = callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                   mode = 'min',\n",
    "                                   patience = 5,\n",
    "                                   restore_best_weights = True)\n",
    "model.compile(loss=loss,\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "107/107 [==============================] - 28s 266ms/step - loss: 0.6905 - accuracy: 0.4502 - val_loss: 0.6873 - val_accuracy: 0.4427\n",
      "Epoch 2/10\n",
      "107/107 [==============================] - 26s 246ms/step - loss: 0.6849 - accuracy: 0.4502 - val_loss: 0.6802 - val_accuracy: 0.4427\n",
      "Epoch 3/10\n",
      "107/107 [==============================] - 26s 244ms/step - loss: 0.6496 - accuracy: 0.5179 - val_loss: 0.6358 - val_accuracy: 0.5587\n",
      "Epoch 4/10\n",
      "107/107 [==============================] - 26s 245ms/step - loss: 0.5769 - accuracy: 0.6665 - val_loss: 0.6086 - val_accuracy: 0.6321\n",
      "Epoch 5/10\n",
      "107/107 [==============================] - 26s 244ms/step - loss: 0.5185 - accuracy: 0.7227 - val_loss: 0.5953 - val_accuracy: 0.6621\n",
      "Epoch 6/10\n",
      "107/107 [==============================] - 26s 244ms/step - loss: 0.4733 - accuracy: 0.7576 - val_loss: 0.6055 - val_accuracy: 0.6659\n",
      "Epoch 7/10\n",
      "107/107 [==============================] - 26s 245ms/step - loss: 0.4370 - accuracy: 0.7781 - val_loss: 0.6265 - val_accuracy: 0.6672\n",
      "Epoch 8/10\n",
      "107/107 [==============================] - 26s 240ms/step - loss: 0.4108 - accuracy: 0.7957 - val_loss: 0.6580 - val_accuracy: 0.6822\n",
      "Epoch 9/10\n",
      "107/107 [==============================] - 25s 235ms/step - loss: 0.3873 - accuracy: 0.8098 - val_loss: 0.6581 - val_accuracy: 0.6747\n",
      "Epoch 10/10\n",
      "107/107 [==============================] - 26s 241ms/step - loss: 0.3633 - accuracy: 0.8229 - val_loss: 0.6933 - val_accuracy: 0.6771\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(trainsents, trainlabels, epochs = 10, batch_size = 128, verbose = 1, validation_split = 0.25, callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/142 [==============================] - 3s 20ms/step - loss: 0.5989 - accuracy: 0.6564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5989043712615967, 0.6563740372657776]"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(testsents, testlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "### vectorization with Spacy pretrained word embeddings ###\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(rawtext)\n",
    "trainseqs = tokenizer.texts_to_sequences(rawtext)\n",
    "word_index = tokenizer.word_index\n",
    "df_text = pd.DataFrame({'rawtext': rawtext})\n",
    "df_text['trainseqs'] = df_text.rawtext.apply(lambda x:tokenizer.texts_to_sequences([x])[0])\n",
    "maxlen = 20 #avg sentence length is approx. 17\n",
    "encoded_sents = pad_sequences(trainseqs, maxlen = maxlen, padding = 'post', truncating = 'post')\n",
    "df_index_word = pd.Series(tokenizer.index_word)\n",
    "df_index_word_valid = df_index_word[:]\n",
    "df_index_word_valid = pd.Series(['place_holder']).append(df_index_word_valid)\n",
    "df_index_word_valid = df_index_word_valid.reset_index()\n",
    "df_index_word_valid.columns = ['tokenid','token']\n",
    "df_index_word_valid['word2vec'] = df_index_word_valid.token.apply(lambda x: nlp(x).vector)\n",
    "df_index_word_valid['is_oov'] = df_index_word_valid.token.apply(lambda x: nlp(x)[0].is_oov)\n",
    "df_index_word_valid.at[0,'word2vec'] = np.zeros_like(df_index_word_valid.at[0,'word2vec'])\n",
    "embed_matrix = np.array([vec for vec in df_index_word_valid.word2vec.values])\n",
    "embed_dim = embed_matrix.shape[1]\n",
    "embed_layer = Embedding(input_dim = 12652, output_dim = embed_dim, embeddings_initializer = Constant(embed_matrix), input_length=maxlen, mask_zero = True, trainable = False)\n",
    "embedded_sents = (embed_layer(encoded_sents)).numpy()\n",
    "embedded_sents = embedded_sents.tolist()\n",
    "for i in range(len(embedded_sents)):\n",
    "    embedded_sents[i].append(labels[i])\n",
    "embedded_sents = np.array(embedded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "### shuffling and creating train/test splits ###\n",
    "np.random.shuffle(embedded_sents)\n",
    "cutoff = int(0.8*len(labels))\n",
    "train_data = embedded_sents[:cutoff, :20]\n",
    "test_data = embedded_sents[cutoff:,:20]\n",
    "\n",
    "train_labels = []\n",
    "for i in range(cutoff):\n",
    "    train_labels.append(embedded_sents[i][20])\n",
    "train_labels = np.array(train_labels)\n",
    "dupl = train_labels\n",
    "train_labels = np.column_stack((train_labels, dupl))\n",
    "\n",
    "for i in range(len(train_labels)):\n",
    "    if train_labels[i][1] == 0:\n",
    "        train_labels[i][0] = 1\n",
    "    if train_labels[i][1] == 1:\n",
    "        train_labels[i][0] = 0\n",
    "\n",
    "test_labels = []\n",
    "for i in range(len(embedded_sents)-cutoff):\n",
    "    test_labels.append(embedded_sents[i+cutoff][20])\n",
    "test_labels = np.array(test_labels)\n",
    "dupl = test_labels\n",
    "test_labels = np.column_stack((test_labels, dupl))\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    if test_labels[i][1] == 0:\n",
    "        test_labels[i][0] = 1\n",
    "    if test_labels[i][1] == 1:\n",
    "        test_labels[i][0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "### adjusting data types to create tensors for input and one-hot encoded labels ###\n",
    "train_sents = []\n",
    "for k in range(len(train_data)):\n",
    "    temp = []\n",
    "    for j in range(20):\n",
    "        for i in range(96):\n",
    "            temp.append(train_data[k][j][i]) \n",
    "    temp = np.array(temp)\n",
    "    train_sents.append(temp)\n",
    "train_sents = np.array(train_sents)\n",
    "\n",
    "test_sents = []\n",
    "for k in range(len(test_data)):\n",
    "    temp = []\n",
    "    for j in range(20):\n",
    "        for i in range(96):\n",
    "            temp.append(test_data[k][j][i]) \n",
    "    temp = np.array(temp)\n",
    "    test_sents.append(temp)\n",
    "test_sents = np.array(test_sents)\n",
    "tdata = tf.convert_to_tensor(train_sents)\n",
    "ldata = tf.convert_to_tensor(train_labels)\n",
    "evalsents = tf.convert_to_tensor(test_sents)\n",
    "evallabels = tf.convert_to_tensor(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating model ###\n",
    "model2 = Sequential()\n",
    "model2.add(Input(shape=(1920,)))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Reshape((1,1920)))\n",
    "model2.add(Bidirectional(tf.keras.layers.LSTM(64)))\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "model2.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "earlystop = callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                   mode = 'min',\n",
    "                                   patience = 5,\n",
    "                                   restore_best_weights = True)\n",
    "model2.compile(loss=loss,\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "107/107 [==============================] - 4s 33ms/step - loss: 0.7101 - accuracy: 0.5698 - val_loss: 0.7059 - val_accuracy: 0.5887\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 3s 28ms/step - loss: 0.6862 - accuracy: 0.6286 - val_loss: 0.7053 - val_accuracy: 0.5805\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 3s 28ms/step - loss: 0.6625 - accuracy: 0.6817 - val_loss: 0.7027 - val_accuracy: 0.5891\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 3s 29ms/step - loss: 0.6340 - accuracy: 0.7437 - val_loss: 0.7030 - val_accuracy: 0.5935\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 3s 29ms/step - loss: 0.6117 - accuracy: 0.7884 - val_loss: 0.7033 - val_accuracy: 0.5902\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 3s 28ms/step - loss: 0.5936 - accuracy: 0.8240 - val_loss: 0.7063 - val_accuracy: 0.5823\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 3s 28ms/step - loss: 0.5808 - accuracy: 0.8493 - val_loss: 0.7061 - val_accuracy: 0.5851\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 3s 27ms/step - loss: 0.5699 - accuracy: 0.8686 - val_loss: 0.7090 - val_accuracy: 0.5783\n"
     ]
    }
   ],
   "source": [
    "history = model2.fit(tdata, ldata, epochs = 100, batch_size = 128, verbose = 1, validation_split = 0.25, callbacks = [earlystop])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
